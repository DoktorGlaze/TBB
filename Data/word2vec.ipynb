{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "# Step 1: Load the JSON file using Pandas and extract the text data\n",
    "data = pd.read_json('Testfil_FINAL.json')\n",
    "description_text = data['description_text']\n",
    "\n",
    "# Step 2: Preprocess the text data\n",
    "processed_data = [simple_preprocess(text) for text in description_text]\n",
    "\n",
    "# Step 3: Train the Word2Vec model\n",
    "model = Word2Vec(processed_data, min_count=1, vector_size=100)\n",
    "\n",
    "# Step 4: Save the trained model\n",
    "model.save('word2vec_model_desc_text')\n",
    "\n",
    "# You can now use the trained model for various tasks, such as word similarity or keyword-in-context searches.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/home/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"Key 'analytisk' not present\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-541688633d72>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mtarget_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"analytisk\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mword_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarget_word\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# Perform KWIC search\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key_or_keys)\u001b[0m\n\u001b[1;32m    402\u001b[0m         \"\"\"\n\u001b[1;32m    403\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey_or_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_KEY_TYPES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 404\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey_or_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkey_or_keys\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mget_vector\u001b[0;34m(self, key, norm)\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m         \"\"\"\n\u001b[0;32m--> 447\u001b[0;31m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    448\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill_norms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mget_index\u001b[0;34m(self, key, default)\u001b[0m\n\u001b[1;32m    419\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 421\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Key '{key}' not present\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"Key 'analytisk' not present\""
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.text import Text\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Preprocess the text and tokenize into sentences\n",
    "#corpus = \"Är du intresserad av kundbehov och hur detta kan skapa bättre användarupplevelser?På FLIR får du möjligheten att arbeta med engagerade kollegor där företagskulturen präglas av öppenhet och prestigelöshet.Vi söker dig som vill arbeta inom User Research där du förväntas vara analytisk och utåtriktad.Sök redan idag då urval sker löpande!Rollen innebär att du ska vara med och identifiera samt utvärdera framtidens produkter och vara en evangelist för detta inom produktutvecklingen på FLIR Systems.Du kommer arbeta nära en kollega som jobbat i rollen under en längre period samt samarbeta med personer i andra roller inom industridesign, interaktionsdesign, produktledning, marknadsföring samt andra ingenjörsdiscipliner.Du kommer utföra analytiska undersökningar och ibland innebär det en del resande, både i Sverige och utomlands.Det gäller ungefär 20-25 dagar om året.Den här rekryteringsprocessen hanteras av Academic Work och FLIRs önskemål är att alla samtal och mail kring tjänsten går till Academic Work Uppdraget är en del av vår personaluthyrning.Du kommer vara anställd av Academic Work och arbeta som konsult hos FLIR.FLIRs intention är att du på sikt ska kunna bli anställd direkt hos dem, förutsatt att alla parter är nöjda med samarbetet.Du utför studier för att identifiera kundgrupper men analyserar även färdiga prototyper för att se så att de stämmer in med vad vald målgrupps behov.En stor del av rollen innebär att förstå kundernas behov, kunna uttrycka behoven samt bidra till att de omsätts i praktiken.Du skapar vidare användarscenarios, användarprofiler och liknande.Kunderna är utspridda runt om i världen och därför innebär jobbet en del resor för att kunna träffa samt intervjua dem.Det är din uppgift att dra slutsatser kring kundbehoven.Det kan handla om att svara på frågor såsom, \\\"Hur ser deras arbetsdag ut?\\\", \\\"Vilka problem möter de i arbetet?\\\", \\\"Hur löser våra produkter deras problem?\\\".Du skissar därefter upp en historia kring användaren som på ett målande samt kreativt sätt beskriver kunden.Dessa kundinsikter levererar du bl.a. vidare till designteamet och produktledning.Din roll är också att kontinuerligt utvärdera prototyper, koncept av både mjukvaror och fysiska produkter, för att se om den är rätt utformad för vald målgrupp.Tjänsten passar dig som söker en roll där du får använda dina analytiska och metodiska kunskaper och samtidigt vara utåtriktad och nyfiken.Här finns möjligheten att vara kreativ, ha inflytande samt arbeta med spännande och högteknologiska produkter med kunden i fokus.VI SÖKER DIG SOM * Har en avklarad utbildning inom psykologi, människa-data interaktion (HCI), beteendevetenskap, interaktionsdesign, eller liknande * Har erfarenhet och kunskap av kvalitativa och kvantitativa metoder * Har intresse för teknik och produktutveckling * Har god förmåga att illustrera och kommunicera ett budskap * Har mycket goda kunskaper i engelska då all dokumentation och interaktion kommer ske i detta språk Det är meriterande om du.* Har kommit i kontakt med produktutveckling, interaktionsdesign eller industridesign * Har erfarenhet av att arbeta med A/B testning, google analytics eller liknande analysverktyg Vi lägger stor vikt vid dina personliga egenskaper för denna tjänst.Som person har du en empatisk förmåga att sätta dig in i andra människors behov kombinerat med en hög känska för kvalite.Du ser dig själv som en analytisk person som kan bidra med inspiration och har lätt för att skapa nya kontaktytor.Du behöver ha förmågan att tänka kritiskt och inte vara rädd för att föreslå förslag till förändringar.Vidare är det viktigt att du är social samt framåt då du kommer behöva ta kontakt med olika människor i olika roller både inom och utanför FLIR.I din roll ingår även att presentera material för kollegor vilket gör att du behöver vara duktig kommunikativt, både i tal och skrift.ÖVRIG INFORMATION * Start: Januari * Omfattning: 6 månader med goda chanser till förlängning * Placering: Täby * Sök tjänsten genom att klicka på Ansök nedan.Vi går igenom urvalet löpande och annonsen kan stängas ner innan tjänsten är tillsatt om vi gått över till urvals och intervjufasen.Skriv annonstiteln i rubriken och kopiera gärna in länken till annonsen i ditt mail.För generella funderingar kring din ansökan/vår rekryteringsprocess finns vi ofta tillgängliga i chatten här på webben under vardagar, hör gärna av dig där!Academic Work är Home of the Young Professionals.Vi vänder oss till dig som är akademiker och fortfarande studerar på högskola/universitet, är nyutexaminerad eller har några års arbetslivserfarenhet.* Vårt erbjudande till dig som heltidskonsult hos Academic Work inkluderar utöver kollektivavtalsenliga villkor och dedikerad konsultchef också rabatter och erbjudanden genom vår konsultportal.Läs mer om vårt erbjudande här SKYDD FÖR DIN PERSONLIGA INTEGRITET När du skickar in din ansökan till Academic Work godkänner och samtycker du till att Academic Work behandlar dina personuppgifter enligt Personuppgiftslagen.Läs mer under vår PuL-sida.\"\n",
    "corpus = 'analytisk'\n",
    "sentences = [word_tokenize(sentence) for sentence in Text(corpus)]\n",
    "\n",
    "# Train Word2Vec model\n",
    "model = Word2Vec(sentences, vector_size=100, window=5, min_count=1)\n",
    "\n",
    "# Get word embeddings\n",
    "target_word = \"analytisk\"\n",
    "\n",
    "word_embedding = model.wv[target_word]\n",
    "\n",
    "# Perform KWIC search\n",
    "context_words = model.wv.similar_by_vector(word_embedding, topn=5)\n",
    "context_words = [word for word, _ in context_words]\n",
    "\n",
    "kwic_results = []\n",
    "for sentence in sentences:\n",
    "    if target_word in sentence:\n",
    "        idx = sentence.index(target_word)\n",
    "        context = \" \".join(sentence[max(0, idx - 5):idx] + sentence[idx + 1:min(idx + 6, len(sentence))])\n",
    "        kwic_results.append((context, sentence))\n",
    "\n",
    "# Print KWIC results\n",
    "for context, sentence in kwic_results:\n",
    "    print(context, \"[\", target_word, \"]\", \" \".join(sentence))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# Step 1: Load the JSON file using Pandas and extract the text data\n",
    "data = pd.read_json('Testfil_FINAL.json')\n",
    "description_text = data['description_text']\n",
    "\n",
    "# Step 2: Tokenize sentences in the text data\n",
    "sentences = []\n",
    "for text in description_text:\n",
    "    text_sentences = sent_tokenize(text)\n",
    "    sentences.extend(text_sentences)\n",
    "\n",
    "# Step 3: Preprocess the text data\n",
    "processed_data = [simple_preprocess(sentence) for sentence in sentences]\n",
    "\n",
    "# Step 4: Train the Word2Vec model\n",
    "model = Word2Vec(processed_data, min_count=1, vector_size=100)\n",
    "\n",
    "# Step 5: Save the trained model\n",
    "model.save('word2vec_model_desc_sentence')\n",
    "\n",
    "# You can now use the trained model for various tasks, such as word similarity or keyword-in-context searches.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sentences.txt', 'w', encoding='utf-8') as file:\n",
    "    for sentence in sentences:\n",
    "        file.write(sentence + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity: 0.93 Sentence: pedagogisk\n",
      "Similarity: 0.91 Sentence: noggrann\n",
      "Similarity: 0.90 Sentence: strukturerad\n",
      "Similarity: 0.90 Sentence: kommunikativ\n",
      "Similarity: 0.88 Sentence: serviceinriktad\n"
     ]
    }
   ],
   "source": [
    "# Load the saved Word2Vec model\n",
    "model = Word2Vec.load('word2vec_model_desc_sentence')\n",
    "\n",
    "# Define a target sentence for similarity comparison\n",
    "target_sentence = \"analytisk\"\n",
    "\n",
    "# Preprocess the target sentence\n",
    "processed_target = simple_preprocess(target_sentence)\n",
    "\n",
    "# Find similar sentences\n",
    "similar_sentences = model.wv.most_similar(positive=processed_target, topn=5)\n",
    "\n",
    "# Print the similar sentences\n",
    "for sentence, similarity in similar_sentences:\n",
    "    print(f\"Similarity: {similarity:.2f} Sentence: {sentence}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['du', 'är', 'diplomatisk', 'har', 'god', 'analytisk', 'förmåga', 'och', 'kan', 'enkelt', 'röra', 'dig', 'mellan', 'detaljer', 'och', 'helheter']\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "At least one of the passed list is empty.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-bf1762c73565>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0msimilar_sentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprocessed_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0msimilarity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessed_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0msimilar_sentences\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msimilarity\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mn_similarity\u001b[0;34m(self, ws1, ws2)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \"\"\"\n\u001b[1;32m   1256\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mws1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mws2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1257\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mZeroDivisionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'At least one of the passed list is empty.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1258\u001b[0m         \u001b[0mmean1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_mean_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mws1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpre_normalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0mmean2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_mean_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mws2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpre_normalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: At least one of the passed list is empty."
     ]
    }
   ],
   "source": [
    "# Load the saved Word2Vec model\n",
    "model = Word2Vec.load('word2vec_model_desc_sentence')\n",
    "\n",
    "# Define a target sentence for similarity comparison\n",
    "target_sentence = \"Du är diplomatisk, har god analytisk förmåga och kan enkelt röra dig mellan detaljer och helheter.\"\n",
    "\n",
    "# Preprocess the target sentence\n",
    "processed_target = simple_preprocess(target_sentence)\n",
    "\n",
    "print(processed_target)\n",
    "\n",
    "# Find similar sentences\n",
    "similar_sentences = []\n",
    "for sentence in processed_data:\n",
    "    similarity = model.wv.n_similarity(processed_target, sentence)\n",
    "    similar_sentences.append((sentence, similarity))\n",
    "\n",
    "# Sort the similar sentences based on similarity score\n",
    "similar_sentences = sorted(similar_sentences, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the top 5 similar sentences\n",
    "for sentence, similarity in similar_sentences[:5]:\n",
    "    original_sentence = ' '.join(sentence)\n",
    "    print(f\"Similarity: {similarity:.2f} Sentence: {original_sentence}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

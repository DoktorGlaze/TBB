{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skapar modell för hela description text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skapar modell för hela description text\n",
    "import pandas as pd\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "# Step 1: Load the JSON file using Pandas and extract the text data\n",
    "data = pd.read_json('Testfil_FINAL.json')\n",
    "description_text = data['description_text']\n",
    "\n",
    "# Step 2: Preprocess the text data\n",
    "processed_data = [simple_preprocess(text) for text in description_text]\n",
    "\n",
    "# Step 3: Train the Word2Vec model\n",
    "model = Word2Vec(processed_data, min_count=1, vector_size=100)\n",
    "\n",
    "# Step 4: Save the trained model\n",
    "model.save('word2vec_model_desc_text')\n",
    "\n",
    "# You can now use the trained model for various tasks, such as word similarity or keyword-in-context searches.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Börja här"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/home/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# skapar modell för enstaka meningar i description text\n",
    "import pandas as pd\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "\n",
    "\n",
    "# Step 1: Load the JSON file using Pandas and extract the text data\n",
    "data = pd.read_json('Testfil_FINAL_2.json')\n",
    "description_text = data['description_text']\n",
    "\n",
    "# Step 2: Tokenize sentences in the text data\n",
    "sentences = []\n",
    "for text in description_text:\n",
    "    text_sentences = sent_tokenize(text)\n",
    "    sentences.extend(text_sentences)\n",
    "\n",
    "# Step 3: Preprocess the text data\n",
    "processed_data = [simple_preprocess(sentence) for sentence in sentences]\n",
    "\n",
    "# Step 4: Train the Word2Vec model\n",
    "model = Word2Vec(processed_data, min_count=1, vector_size=100)\n",
    "\n",
    "# Step 5: Save the trained model\n",
    "model.save('word2vec_model_desc_sentence')\n",
    "\n",
    "# You can now use the trained model for various tasks, such as word similarity or keyword-in-context searches.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alternativ kod som tar bort stopwords i processed_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\kevin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\kevin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"# Step 4: Train the Word2Vec model\\nmodel = Word2Vec(processed_data, min_count=1, vector_size=100)\\n\\n# Step 5: Save the trained model\\nmodel.save('word2vec_model_desc_sentence')\\n\\n# You can now use the trained model for various tasks, such as word similarity or keyword-in-context searches.\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Step 1: Load the JSON file using Pandas and extract the text data\n",
    "data = pd.read_json('Testfil_FINAL.json')\n",
    "description_text = data['description_text']\n",
    "\n",
    "# Step 2: Tokenize sentences in the text data\n",
    "sentences = []\n",
    "for text in description_text:\n",
    "    text_sentences = sent_tokenize(text)\n",
    "    sentences.extend(text_sentences)\n",
    "\n",
    "# Step 3: Preprocess the text data and remove stopwords\n",
    "stop_words = set(stopwords.words('swedish'))  # Load Swedish stopwords from NLTK\n",
    "\n",
    "processed_data = []\n",
    "for sentence in sentences:\n",
    "    # Tokenize and filter out stopwords\n",
    "    tokens = simple_preprocess(sentence)\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    processed_data.append(filtered_tokens)\n",
    "\n",
    "'''# Step 4: Train the Word2Vec model\n",
    "model = Word2Vec(processed_data, min_count=1, vector_size=100)\n",
    "\n",
    "# Step 5: Save the trained model\n",
    "model.save('word2vec_model_desc_sentence')\n",
    "\n",
    "# You can now use the trained model for various tasks, such as word similarity or keyword-in-context searches.'''\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Söker igenom alla meningar och utan ord prio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['du', 'är', 'diplomatisk', 'har', 'god', 'analytisk', 'förmåga', 'och', 'kan', 'enkelt', 'röra', 'dig', 'mellan', 'detaljer', 'och', 'helheter']\n",
      "Similarity: 1.00 Sentence: du är diplomatisk har god analytisk förmåga och kan enkelt röra dig mellan detaljer och helheter\n",
      "Similarity: 0.94 Sentence: du är bra på problemlösning och har god analytisk förmåga\n",
      "Similarity: 0.94 Sentence: du är analytisk och har en god förmåga att prioritera\n",
      "Similarity: 0.94 Sentence: god kommunikativ och social förmåga samt ordning och reda är starka kort hos dig\n",
      "Similarity: 0.94 Sentence: du är serviceinriktad noggrann ansvarskännande och har god förmåga att kommunicera med din omgivning\n",
      "Similarity: 0.94 Sentence: du är strukturerad självgående och har förmåga att prioritera\n",
      "Similarity: 0.94 Sentence: du är driven och ansvarstagande och har god kommunikativ förmåga\n",
      "Similarity: 0.93 Sentence: är du drivande och strukturerad och har förmåga att samarbeta\n",
      "Similarity: 0.93 Sentence: självfallet är du ansvarstagande noggrann och har förmågan att prioritera\n",
      "Similarity: 0.93 Sentence: du har ett stort eget driv är självgående och har god förmåga att planera samtidigt som du är utåtriktad kommunikativ och har lätt för att uttrycka dig\n"
     ]
    }
   ],
   "source": [
    "# Load the saved Word2Vec model\n",
    "model = Word2Vec.load('word2vec_model_desc_sentence')\n",
    "\n",
    "# Define a target sentence for similarity comparison\n",
    "target_sentence = \"Du är diplomatisk, har god analytisk förmåga och kan enkelt röra dig mellan detaljer och helheter.\"\n",
    "\n",
    "# Preprocess the target sentence\n",
    "processed_target = simple_preprocess(target_sentence)\n",
    "\n",
    "print(processed_target)\n",
    "\n",
    "# Find similar sentences\n",
    "similar_sentences = []\n",
    "for sentence in processed_data:\n",
    "    if len(sentence) > 0:\n",
    "        similarity = model.wv.n_similarity(processed_target, sentence)\n",
    "        similar_sentences.append((sentence, similarity))\n",
    "\n",
    "# Sort the similar sentences based on similarity score\n",
    "similar_sentences = sorted(similar_sentences, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the top 5 similar sentences\n",
    "for sentence, similarity in similar_sentences[:10]:\n",
    "    original_sentence = ' '.join(sentence)\n",
    "    print(f\"Similarity: {similarity:.2f} Sentence: {original_sentence}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRIORITERAR ETT ORD FÖR VEKTORN SÅ ALLA MENINGAR UTAN ORDET EXKLUDERAS FRÅN SIMILARITETEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['du', 'är', 'diplomatisk', 'har', 'god', 'analytisk', 'förmåga', 'och', 'kan', 'enkelt', 'röra', 'dig', 'mellan', 'detaljer', 'och', 'helheter']\n",
      "Similarity: 1.00 Sentence: du är diplomatisk har god analytisk förmåga och kan enkelt röra dig mellan detaljer och helheter\n",
      "Similarity: 0.94 Sentence: du är bra på problemlösning och har god analytisk förmåga\n",
      "Similarity: 0.94 Sentence: du är analytisk och har en god förmåga att prioritera\n",
      "Similarity: 0.93 Sentence: vad vi vill att du har med dig har god analytisk kapacitet till helhetstänk har god förmåga att uttrycka dig väl tal och skrift har en god social kompetens har bra förmåga att arbeta såväl grupp som självständigt sätter kunden fokus och ge dem god service är serviceinriktad stresstålig noggrann strukturerad och ansvarsfull meriterande är om du har tidigare erfarenhet av oas äga ik collekt melos softronic är ett företag med såväl bredd som djup\n",
      "Similarity: 0.92 Sentence: som person är du strukturerad har god analytisk förmåga och förståelse för komplexa flöden och beroendesamband\n",
      "Similarity: 0.92 Sentence: som person är du både noggrann strukturerad och analytisk samt har lätt för att se samband och förstå helheten\n",
      "Similarity: 0.92 Sentence: du är analytisk och har en god förmåga att lösa komplexa problem\n",
      "Similarity: 0.92 Sentence: du bör ha god analytisk förmåga och kunna tänka utanför boxen\n",
      "Similarity: 0.91 Sentence: vi söker dig som är analytisk strukturerad och noggrann med en god förmåga till överblick och sätta saker sitt rätta sammanhang\n",
      "Similarity: 0.91 Sentence: vidare drivs du av problemlösning och har en god analytisk förmåga\n"
     ]
    }
   ],
   "source": [
    "# Load the saved Word2Vec model\n",
    "model = Word2Vec.load('word2vec_model_desc_sentence')\n",
    "\n",
    "# Define a target sentence for similarity comparison\n",
    "target_sentence = \"Du är diplomatisk, har god analytisk förmåga och kan enkelt röra dig mellan detaljer och helheter.\"\n",
    "\n",
    "# Preprocess the target sentence\n",
    "processed_target = simple_preprocess(target_sentence)\n",
    "\n",
    "print(processed_target)\n",
    "\n",
    "# Find similar sentences\n",
    "similar_sentences = []\n",
    "for sentence in processed_data:\n",
    "    if len(sentence) > 0 and 'analytisk' in sentence:  # Check if 'priority_word' is present in the sentence\n",
    "        similarity = model.wv.n_similarity(processed_target, sentence)\n",
    "        similar_sentences.append((sentence, similarity))\n",
    "\n",
    "# Sort the similar sentences based on similarity score\n",
    "similar_sentences = sorted(similar_sentences, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the top 5 similar sentences\n",
    "for sentence, similarity in similar_sentences[:10]:\n",
    "    original_sentence = ' '.join(sentence)\n",
    "    print(f\"Similarity: {similarity:.2f} Sentence: {original_sentence}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic analys på alla rader i dataset. Inte relevant då den inte tar bort stoppord och inte filtrerar på meningar med dåliga ord."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "\n",
    "# Preprocess the sentences\n",
    "#processed_data = [...]  # Your preprocessed sentences\n",
    "\n",
    "# Create a dictionary from the preprocessed data\n",
    "dictionary = corpora.Dictionary(processed_data)\n",
    "\n",
    "# Create a corpus (bag of words representation) using the dictionary\n",
    "corpus = [dictionary.doc2bow(sentence) for sentence in processed_data]\n",
    "\n",
    "# Train the LDA model\n",
    "num_topics = 5  # Specify the number of topics\n",
    "lda_model = models.LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=10)\n",
    "\n",
    "# Print the topics and their top keywords\n",
    "topics = lda_model.print_topics(num_words=5)\n",
    "for topic in topics:\n",
    "    print(topic)\n",
    "\n",
    "# Get the topic distribution for each sentence in the dataset\n",
    "topic_distributions = []\n",
    "for sentence in processed_data:\n",
    "    bow = dictionary.doc2bow(sentence)\n",
    "    topic_distribution = lda_model.get_document_topics(bow)\n",
    "    topic_distributions.append(topic_distribution)\n",
    "\n",
    "# Visualize the dominant topic for each sentence\n",
    "for i, topic_dist in enumerate(topic_distributions):\n",
    "    dominant_topic = max(topic_dist, key=lambda x: x[1])\n",
    "    print(f\"Sentence {i+1}: Dominant Topic - {dominant_topic[0]}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Genererar mening utifrån best guess på efterkommande ord\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Sentence: man vara asd ha hög pedagogisk förmågan samt måste ladda befinna sig utgångspunkten helheten samt helheten\n"
     ]
    }
   ],
   "source": [
    "# Load the saved Word2Vec model\n",
    "model = Word2Vec.load('word2vec_model_desc_sentence')\n",
    "\n",
    "# Define a target sentence for similarity comparison\n",
    "target_sentence = \"Du är diplomatisk, har god analytisk förmåga och kan enkelt röra dig mellan detaljer och helheter.\"\n",
    "\n",
    "# Preprocess the target sentence\n",
    "processed_target = simple_preprocess(target_sentence)\n",
    "\n",
    "# Initialize an empty list to store the generated sentence\n",
    "generated_sentence = []\n",
    "\n",
    "# Iterate over each word in the target sentence\n",
    "for target_word in processed_target:\n",
    "    # Check if the target word is in the vocabulary of the Word2Vec model\n",
    "    if target_word in model.wv:\n",
    "        # Get the most similar word to the target word\n",
    "        similar_word = model.wv.most_similar(positive=[target_word], topn=1)[0][0]\n",
    "        generated_sentence.append(similar_word)\n",
    "\n",
    "# Print the generated sentence\n",
    "generated_sentence = ' '.join(generated_sentence)\n",
    "print(f\"Generated Sentence: {generated_sentence}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/home/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['diplomatisk', 'god', 'analytisk', 'förmåga', 'enkelt', 'röra', 'detaljer', 'helheter']\n",
      "Similarity: 1.00 Sentence: du är diplomatisk har god analytisk förmåga och kan enkelt röra dig mellan detaljer och helheter\n",
      "Similarity: 0.97 Sentence: du är analytisk och har en god förmåga att prioritera\n",
      "Similarity: 0.96 Sentence: god analytisk förmåga\n",
      "Similarity: 0.96 Sentence: vidare har du god analytisk förmåga\n",
      "Similarity: 0.95 Sentence: du är bra på problemlösning och har god analytisk förmåga\n",
      "Similarity: 0.94 Sentence: din yrkesroll är du serviceminded noggrann strukturerad samt med en god analytisk förmåga\n",
      "Similarity: 0.93 Sentence: vidare drivs du av problemlösning och har en god analytisk förmåga\n",
      "Similarity: 0.92 Sentence: du har en mycket god analytisk förmåga och är pedagogisk uthållig och har hög integritet\n",
      "Similarity: 0.92 Sentence: som person är du strukturerad har god analytisk förmåga och förståelse för komplexa flöden och beroendesamband\n",
      "Similarity: 0.92 Sentence: du bör ha god analytisk förmåga och kunna tänka utanför boxen\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load the saved Word2Vec model\n",
    "model = Word2Vec.load('word2vec_model_desc_sentence')\n",
    "\n",
    "# Define a target sentence for similarity comparison\n",
    "target_sentence = \"Du är diplomatisk, har god analytisk förmåga och kan enkelt röra dig mellan detaljer och helheter.\"\n",
    "\n",
    "# Preprocess the target sentence\n",
    "stop_words = set(stopwords.words('swedish'))  # Load Swedish stop words from NLTK\n",
    "processed_target = [word for word in simple_preprocess(target_sentence) if word not in stop_words]\n",
    "\n",
    "print(processed_target)\n",
    "\n",
    "# Find similar sentences\n",
    "similar_sentences = []\n",
    "for sentence in processed_data:\n",
    "    # Remove stop words from the sentence\n",
    "    processed_sentence = [word for word in sentence if word not in stop_words]\n",
    "    if len(sentence) > 0 and 'analytisk' in sentence:  # Check if 'priority_word' is present in the sentence\n",
    "        similarity = model.wv.n_similarity(processed_target, processed_sentence)\n",
    "        similar_sentences.append((sentence, similarity))\n",
    "\n",
    "\n",
    "\n",
    "# Sort the similar sentences based on similarity score\n",
    "similar_sentences = sorted(similar_sentences, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the top 5 similar sentences\n",
    "for sentence, similarity in similar_sentences[:10]:\n",
    "    original_sentence = ' '.join(sentence)\n",
    "    print(f\"Similarity: {similarity:.2f} Sentence: {original_sentence}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TOPICS UTIFRÅN MENINGAR SOM INNEHÅLLER TARGET WORD. FUNGERAR SÅDÄR DÅ DEN BEHÖVER EN TARGET SENTENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\kevin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\kevin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity: 1.00 Sentence: diplomatisk god analytisk förmåga enkelt röra detaljer helheter\n",
      "Similarity: 1.00 Sentence: god analytisk förmåga\n",
      "Similarity: 1.00 Sentence: analytisk god förmåga prioritera\n",
      "Similarity: 1.00 Sentence: vidare god analytisk förmåga\n",
      "Similarity: 1.00 Sentence: vidare förmåga hantera stora mängder information kombination god analytisk förmåga\n",
      "Similarity: 1.00 Sentence: vidare förmåga hantera stora mängder information kombination god analytisk förmåga\n",
      "Similarity: 1.00 Sentence: person positiv attityd arbete god analytisk förmåga\n",
      "Similarity: 1.00 Sentence: person ser god analytisk förmåga framgångsrikt analysera bedöma it händelser\n",
      "Similarity: 1.00 Sentence: tekniska miljön komplex kräver god analytisk förmåga samt kunna beskriva tekniska problem slutanvändaren vardagligt sätt\n",
      "Similarity: 1.00 Sentence: tycker lösa problem god analytisk förmåga\n",
      "Sentence 1: Dominant Topic - 0\n",
      "Sentence 2: Dominant Topic - 1\n",
      "Sentence 3: Dominant Topic - 2\n",
      "Sentence 4: Dominant Topic - 2\n",
      "Sentence 5: Dominant Topic - 2\n",
      "Sentence 6: Dominant Topic - 2\n",
      "Sentence 7: Dominant Topic - 2\n",
      "Sentence 8: Dominant Topic - 1\n",
      "Sentence 9: Dominant Topic - 2\n",
      "Sentence 10: Dominant Topic - 0\n",
      "Sentence 11: Dominant Topic - 1\n",
      "Sentence 12: Dominant Topic - 1\n",
      "Sentence 13: Dominant Topic - 1\n",
      "Sentence 14: Dominant Topic - 1\n",
      "Sentence 15: Dominant Topic - 1\n",
      "Sentence 16: Dominant Topic - 0\n",
      "Sentence 17: Dominant Topic - 1\n",
      "Sentence 18: Dominant Topic - 1\n",
      "Sentence 19: Dominant Topic - 1\n",
      "Sentence 20: Dominant Topic - 1\n",
      "Sentence 21: Dominant Topic - 1\n",
      "Sentence 22: Dominant Topic - 2\n",
      "Sentence 23: Dominant Topic - 1\n",
      "Sentence 24: Dominant Topic - 0\n",
      "Sentence 25: Dominant Topic - 1\n",
      "Sentence 26: Dominant Topic - 2\n",
      "Sentence 27: Dominant Topic - 0\n",
      "Sentence 28: Dominant Topic - 1\n",
      "Sentence 29: Dominant Topic - 2\n",
      "Sentence 30: Dominant Topic - 2\n",
      "Sentence 31: Dominant Topic - 2\n",
      "Sentence 32: Dominant Topic - 0\n",
      "Sentence 33: Dominant Topic - 1\n",
      "Sentence 34: Dominant Topic - 1\n",
      "Sentence 35: Dominant Topic - 0\n",
      "Sentence 36: Dominant Topic - 0\n",
      "Sentence 37: Dominant Topic - 2\n",
      "Sentence 38: Dominant Topic - 2\n",
      "Sentence 39: Dominant Topic - 2\n",
      "Sentence 40: Dominant Topic - 2\n",
      "Sentence 41: Dominant Topic - 1\n",
      "Sentence 42: Dominant Topic - 2\n",
      "Sentence 43: Dominant Topic - 1\n",
      "Sentence 44: Dominant Topic - 2\n",
      "Sentence 45: Dominant Topic - 0\n",
      "Sentence 46: Dominant Topic - 2\n",
      "Sentence 47: Dominant Topic - 1\n",
      "Sentence 48: Dominant Topic - 1\n",
      "Sentence 49: Dominant Topic - 2\n",
      "Sentence 50: Dominant Topic - 0\n",
      "Sentence 51: Dominant Topic - 1\n",
      "Sentence 52: Dominant Topic - 0\n",
      "Sentence 53: Dominant Topic - 0\n",
      "Sentence 54: Dominant Topic - 2\n",
      "Sentence 55: Dominant Topic - 1\n",
      "Sentence 56: Dominant Topic - 0\n",
      "Sentence 57: Dominant Topic - 2\n",
      "Sentence 58: Dominant Topic - 2\n",
      "Sentence 59: Dominant Topic - 2\n",
      "Sentence 60: Dominant Topic - 2\n",
      "Sentence 61: Dominant Topic - 1\n",
      "Sentence 62: Dominant Topic - 0\n",
      "Sentence 63: Dominant Topic - 2\n",
      "Sentence 64: Dominant Topic - 0\n",
      "Sentence 65: Dominant Topic - 0\n",
      "Sentence 66: Dominant Topic - 2\n",
      "Sentence 67: Dominant Topic - 0\n",
      "Sentence 68: Dominant Topic - 2\n",
      "Sentence 69: Dominant Topic - 1\n",
      "Sentence 70: Dominant Topic - 1\n",
      "Sentence 71: Dominant Topic - 2\n",
      "Sentence 72: Dominant Topic - 0\n",
      "Sentence 73: Dominant Topic - 0\n",
      "Sentence 74: Dominant Topic - 0\n",
      "Sentence 75: Dominant Topic - 2\n",
      "Sentence 76: Dominant Topic - 1\n",
      "Sentence 77: Dominant Topic - 1\n",
      "Sentence 78: Dominant Topic - 1\n",
      "Sentence 79: Dominant Topic - 2\n",
      "Sentence 80: Dominant Topic - 1\n",
      "Sentence 81: Dominant Topic - 1\n",
      "Sentence 82: Dominant Topic - 2\n",
      "Sentence 83: Dominant Topic - 2\n",
      "Sentence 84: Dominant Topic - 1\n",
      "Sentence 85: Dominant Topic - 1\n",
      "Sentence 86: Dominant Topic - 2\n",
      "Sentence 87: Dominant Topic - 2\n",
      "Sentence 88: Dominant Topic - 0\n",
      "Sentence 89: Dominant Topic - 0\n",
      "Sentence 90: Dominant Topic - 2\n",
      "Sentence 91: Dominant Topic - 0\n",
      "Sentence 92: Dominant Topic - 1\n",
      "Sentence 93: Dominant Topic - 0\n",
      "Sentence 94: Dominant Topic - 2\n",
      "Sentence 95: Dominant Topic - 0\n",
      "Sentence 96: Dominant Topic - 2\n",
      "Sentence 97: Dominant Topic - 2\n",
      "Sentence 98: Dominant Topic - 1\n",
      "Sentence 99: Dominant Topic - 2\n",
      "Sentence 100: Dominant Topic - 2\n",
      "Sentence 101: Dominant Topic - 1\n",
      "Sentence 102: Dominant Topic - 1\n",
      "Sentence 103: Dominant Topic - 2\n",
      "Sentence 104: Dominant Topic - 1\n",
      "Sentence 105: Dominant Topic - 2\n",
      "Sentence 106: Dominant Topic - 1\n",
      "Sentence 107: Dominant Topic - 2\n",
      "Sentence 108: Dominant Topic - 2\n",
      "Sentence 109: Dominant Topic - 2\n",
      "Sentence 110: Dominant Topic - 1\n",
      "Sentence 111: Dominant Topic - 2\n",
      "Sentence 112: Dominant Topic - 2\n",
      "Sentence 113: Dominant Topic - 2\n",
      "Sentence 114: Dominant Topic - 0\n",
      "Sentence 115: Dominant Topic - 2\n",
      "Sentence 116: Dominant Topic - 1\n",
      "Sentence 117: Dominant Topic - 2\n",
      "Sentence 118: Dominant Topic - 1\n",
      "Sentence 119: Dominant Topic - 2\n",
      "Sentence 120: Dominant Topic - 2\n",
      "Sentence 121: Dominant Topic - 2\n",
      "Sentence 122: Dominant Topic - 2\n",
      "Sentence 123: Dominant Topic - 2\n",
      "Sentence 124: Dominant Topic - 2\n",
      "Sentence 125: Dominant Topic - 2\n",
      "Sentence 126: Dominant Topic - 2\n",
      "Sentence 127: Dominant Topic - 2\n",
      "Sentence 128: Dominant Topic - 2\n",
      "Sentence 129: Dominant Topic - 2\n",
      "Sentence 130: Dominant Topic - 2\n",
      "Sentence 131: Dominant Topic - 2\n",
      "Sentence 132: Dominant Topic - 2\n",
      "Sentence 133: Dominant Topic - 2\n",
      "Sentence 134: Dominant Topic - 1\n",
      "Sentence 135: Dominant Topic - 2\n",
      "Sentence 136: Dominant Topic - 2\n",
      "Sentence 137: Dominant Topic - 2\n",
      "Sentence 138: Dominant Topic - 2\n",
      "Sentence 139: Dominant Topic - 2\n",
      "Sentence 140: Dominant Topic - 2\n",
      "Sentence 141: Dominant Topic - 2\n",
      "Sentence 142: Dominant Topic - 2\n",
      "Sentence 143: Dominant Topic - 2\n",
      "Sentence 144: Dominant Topic - 2\n",
      "Sentence 145: Dominant Topic - 1\n",
      "Sentence 146: Dominant Topic - 2\n",
      "Sentence 147: Dominant Topic - 1\n",
      "Sentence 148: Dominant Topic - 2\n",
      "Sentence 149: Dominant Topic - 2\n",
      "Sentence 150: Dominant Topic - 2\n",
      "Sentence 151: Dominant Topic - 2\n",
      "Sentence 152: Dominant Topic - 0\n",
      "Sentence 153: Dominant Topic - 1\n",
      "Sentence 154: Dominant Topic - 1\n",
      "Sentence 155: Dominant Topic - 1\n",
      "Sentence 156: Dominant Topic - 2\n",
      "Sentence 157: Dominant Topic - 2\n",
      "Sentence 158: Dominant Topic - 2\n",
      "Sentence 159: Dominant Topic - 2\n",
      "Sentence 160: Dominant Topic - 2\n",
      "Sentence 161: Dominant Topic - 2\n",
      "Sentence 162: Dominant Topic - 1\n",
      "Sentence 163: Dominant Topic - 2\n",
      "Sentence 164: Dominant Topic - 1\n",
      "Sentence 165: Dominant Topic - 2\n",
      "Sentence 166: Dominant Topic - 2\n",
      "Sentence 167: Dominant Topic - 2\n",
      "Sentence 168: Dominant Topic - 2\n",
      "Sentence 169: Dominant Topic - 2\n",
      "Sentence 170: Dominant Topic - 2\n",
      "Sentence 171: Dominant Topic - 2\n",
      "Sentence 172: Dominant Topic - 2\n",
      "Sentence 173: Dominant Topic - 2\n",
      "Sentence 174: Dominant Topic - 2\n",
      "Sentence 175: Dominant Topic - 2\n",
      "Sentence 176: Dominant Topic - 2\n",
      "Sentence 177: Dominant Topic - 2\n",
      "Sentence 178: Dominant Topic - 2\n",
      "Sentence 179: Dominant Topic - 2\n",
      "Sentence 180: Dominant Topic - 2\n",
      "Sentence 181: Dominant Topic - 2\n",
      "Sentence 182: Dominant Topic - 2\n",
      "Sentence 183: Dominant Topic - 2\n",
      "Sentence 184: Dominant Topic - 2\n",
      "Sentence 185: Dominant Topic - 2\n",
      "Sentence 186: Dominant Topic - 2\n",
      "Sentence 187: Dominant Topic - 2\n",
      "Sentence 188: Dominant Topic - 2\n",
      "Sentence 189: Dominant Topic - 2\n",
      "Sentence 190: Dominant Topic - 1\n",
      "Sentence 191: Dominant Topic - 1\n",
      "Sentence 192: Dominant Topic - 1\n",
      "Sentence 193: Dominant Topic - 2\n",
      "Sentence 194: Dominant Topic - 2\n",
      "Sentence 195: Dominant Topic - 1\n",
      "Sentence 196: Dominant Topic - 2\n",
      "Sentence 197: Dominant Topic - 0\n",
      "Sentence 198: Dominant Topic - 0\n",
      "Sentence 199: Dominant Topic - 0\n",
      "Sentence 200: Dominant Topic - 2\n",
      "Sentence 201: Dominant Topic - 2\n",
      "Sentence 202: Dominant Topic - 2\n",
      "Sentence 203: Dominant Topic - 2\n",
      "Sentence 204: Dominant Topic - 2\n",
      "Sentence 205: Dominant Topic - 2\n",
      "Sentence 206: Dominant Topic - 1\n",
      "Sentence 207: Dominant Topic - 2\n",
      "Sentence 208: Dominant Topic - 2\n",
      "Sentence 209: Dominant Topic - 2\n",
      "Sentence 210: Dominant Topic - 1\n",
      "Sentence 211: Dominant Topic - 2\n",
      "Sentence 212: Dominant Topic - 2\n",
      "Sentence 213: Dominant Topic - 1\n",
      "Sentence 214: Dominant Topic - 2\n",
      "Sentence 215: Dominant Topic - 2\n",
      "Sentence 216: Dominant Topic - 2\n",
      "Sentence 217: Dominant Topic - 1\n",
      "Sentence 218: Dominant Topic - 1\n",
      "Sentence 219: Dominant Topic - 1\n",
      "Sentence 220: Dominant Topic - 2\n",
      "Sentence 221: Dominant Topic - 2\n",
      "Sentence 222: Dominant Topic - 2\n",
      "Sentence 223: Dominant Topic - 2\n",
      "Sentence 224: Dominant Topic - 2\n",
      "Sentence 225: Dominant Topic - 2\n",
      "Sentence 226: Dominant Topic - 2\n",
      "Sentence 227: Dominant Topic - 0\n",
      "Sentence 228: Dominant Topic - 2\n",
      "Sentence 229: Dominant Topic - 2\n",
      "Sentence 230: Dominant Topic - 2\n",
      "Sentence 231: Dominant Topic - 2\n",
      "Sentence 232: Dominant Topic - 1\n",
      "Sentence 233: Dominant Topic - 1\n",
      "Sentence 234: Dominant Topic - 2\n",
      "Sentence 235: Dominant Topic - 2\n",
      "Sentence 236: Dominant Topic - 2\n",
      "Sentence 237: Dominant Topic - 2\n",
      "Sentence 238: Dominant Topic - 2\n",
      "Sentence 239: Dominant Topic - 2\n",
      "Sentence 240: Dominant Topic - 2\n",
      "Sentence 241: Dominant Topic - 2\n",
      "Sentence 242: Dominant Topic - 1\n",
      "Sentence 243: Dominant Topic - 2\n",
      "Sentence 244: Dominant Topic - 2\n",
      "Sentence 245: Dominant Topic - 2\n",
      "Sentence 246: Dominant Topic - 0\n",
      "Sentence 247: Dominant Topic - 2\n",
      "Sentence 248: Dominant Topic - 2\n",
      "Sentence 249: Dominant Topic - 2\n",
      "Sentence 250: Dominant Topic - 2\n",
      "Sentence 251: Dominant Topic - 2\n",
      "Sentence 252: Dominant Topic - 2\n",
      "Sentence 253: Dominant Topic - 2\n",
      "Sentence 254: Dominant Topic - 2\n",
      "Sentence 255: Dominant Topic - 2\n",
      "Sentence 256: Dominant Topic - 2\n",
      "Sentence 257: Dominant Topic - 2\n",
      "Sentence 258: Dominant Topic - 2\n",
      "Sentence 259: Dominant Topic - 2\n",
      "Sentence 260: Dominant Topic - 2\n",
      "Sentence 261: Dominant Topic - 2\n",
      "Sentence 262: Dominant Topic - 2\n",
      "Sentence 263: Dominant Topic - 2\n",
      "Sentence 264: Dominant Topic - 2\n",
      "Sentence 265: Dominant Topic - 2\n",
      "Sentence 266: Dominant Topic - 2\n",
      "Sentence 267: Dominant Topic - 2\n",
      "Sentence 268: Dominant Topic - 2\n",
      "Sentence 269: Dominant Topic - 1\n",
      "Sentence 270: Dominant Topic - 2\n",
      "Sentence 271: Dominant Topic - 2\n",
      "Sentence 272: Dominant Topic - 1\n",
      "Sentence 273: Dominant Topic - 1\n",
      "Sentence 274: Dominant Topic - 2\n",
      "Sentence 275: Dominant Topic - 2\n",
      "Sentence 276: Dominant Topic - 2\n",
      "Sentence 277: Dominant Topic - 2\n",
      "Sentence 278: Dominant Topic - 2\n",
      "Sentence 279: Dominant Topic - 2\n",
      "Sentence 280: Dominant Topic - 2\n",
      "Sentence 281: Dominant Topic - 2\n",
      "Sentence 282: Dominant Topic - 2\n",
      "Sentence 283: Dominant Topic - 2\n",
      "Sentence 284: Dominant Topic - 0\n",
      "Sentence 285: Dominant Topic - 2\n",
      "Sentence 286: Dominant Topic - 2\n",
      "Sentence 287: Dominant Topic - 2\n",
      "Sentence 288: Dominant Topic - 2\n",
      "Sentence 289: Dominant Topic - 2\n",
      "Sentence 290: Dominant Topic - 2\n",
      "Sentence 291: Dominant Topic - 2\n",
      "Sentence 292: Dominant Topic - 2\n",
      "Sentence 293: Dominant Topic - 2\n",
      "Sentence 294: Dominant Topic - 2\n",
      "Sentence 295: Dominant Topic - 1\n",
      "Sentence 296: Dominant Topic - 2\n",
      "Sentence 297: Dominant Topic - 2\n",
      "Sentence 298: Dominant Topic - 1\n",
      "Sentence 299: Dominant Topic - 2\n",
      "Sentence 300: Dominant Topic - 2\n",
      "Sentence 301: Dominant Topic - 1\n",
      "Sentence 302: Dominant Topic - 1\n",
      "Sentence 303: Dominant Topic - 2\n",
      "Sentence 304: Dominant Topic - 2\n",
      "Sentence 305: Dominant Topic - 1\n",
      "Sentence 306: Dominant Topic - 2\n",
      "Sentence 307: Dominant Topic - 2\n",
      "Sentence 308: Dominant Topic - 2\n",
      "Sentence 309: Dominant Topic - 2\n",
      "Sentence 310: Dominant Topic - 2\n",
      "Sentence 311: Dominant Topic - 2\n",
      "Sentence 312: Dominant Topic - 2\n",
      "Sentence 313: Dominant Topic - 1\n",
      "Sentence 314: Dominant Topic - 1\n",
      "Sentence 315: Dominant Topic - 2\n",
      "Sentence 316: Dominant Topic - 1\n",
      "Sentence 317: Dominant Topic - 2\n",
      "Sentence 318: Dominant Topic - 1\n",
      "Sentence 319: Dominant Topic - 1\n",
      "Sentence 320: Dominant Topic - 2\n",
      "Sentence 321: Dominant Topic - 2\n",
      "Sentence 322: Dominant Topic - 1\n",
      "Sentence 323: Dominant Topic - 2\n",
      "Sentence 324: Dominant Topic - 2\n",
      "Sentence 325: Dominant Topic - 2\n",
      "Sentence 326: Dominant Topic - 0\n",
      "Sentence 327: Dominant Topic - 2\n",
      "Sentence 328: Dominant Topic - 2\n",
      "Sentence 329: Dominant Topic - 2\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "from nltk.corpus import stopwords\n",
    "from gensim import corpora, models\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load the saved Word2Vec model\n",
    "model = Word2Vec.load('word2vec_model_desc_sentence')\n",
    "\n",
    "# Define a target sentence for similarity comparison\n",
    "target_sentence = \"Du är diplomatisk, har god analytisk förmåga och kan enkelt röra dig mellan detaljer och helheter.\"\n",
    "\n",
    "# Preprocess the target sentence\n",
    "stop_words = set(stopwords.words('swedish'))  # Load Swedish stop words from NLTK\n",
    "processed_target = [word for word in simple_preprocess(target_sentence) if word not in stop_words]\n",
    "\n",
    "# Tokenize sentences in the text data\n",
    "sentences = []\n",
    "for text in description_text:\n",
    "    text_sentences = sent_tokenize(text)\n",
    "    sentences.extend(text_sentences)\n",
    "\n",
    "# Preprocess the sentences and remove stop words\n",
    "processed_data = []\n",
    "for sentence in sentences:\n",
    "    processed_sentence = [word for word in simple_preprocess(sentence) if word not in stop_words]\n",
    "    processed_data.append(processed_sentence)\n",
    "\n",
    "# Filter the sentences to consider only those containing 'analytisk'\n",
    "analytisk_sentences = [sentence for sentence in processed_data if 'analytisk' in sentence]\n",
    "\n",
    "# Train the Word2Vec model\n",
    "model = Word2Vec(analytisk_sentences, min_count=1, vector_size=100)\n",
    "\n",
    "# Find similar sentences\n",
    "similar_sentences = []\n",
    "for sentence in analytisk_sentences:\n",
    "    if len(sentence) > 0:\n",
    "        similarity = model.wv.n_similarity(processed_target, sentence)\n",
    "        similar_sentences.append((sentence, similarity))\n",
    "\n",
    "# Sort the similar sentences based on similarity score\n",
    "similar_sentences = sorted(similar_sentences, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the top 5 similar sentences\n",
    "for sentence, similarity in similar_sentences[:10]:\n",
    "    original_sentence = ' '.join(sentence)\n",
    "    print(f\"Similarity: {similarity:.2f} Sentence: {original_sentence}\")\n",
    "\n",
    "# Remove stop words from the preprocessed data\n",
    "processed_data_no_stopwords = [[word for word in sentence if word not in stop_words] for sentence in analytisk_sentences]\n",
    "\n",
    "# Create a dictionary from the preprocessed data\n",
    "dictionary = corpora.Dictionary(processed_data_no_stopwords)\n",
    "\n",
    "# Create a corpus (bag of words representation) using the dictionary\n",
    "corpus = [dictionary.doc2bow(sentence) for sentence in processed_data_no_stopwords]\n",
    "\n",
    "# Train the LDA model\n",
    "num_topics = 3  # Specify the number of topics\n",
    "lda_model = models.LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=10)\n",
    "\n",
    "# Get the topic distribution for each sentence in the dataset\n",
    "topic_distributions = []\n",
    "for sentence in processed_data_no_stopwords:\n",
    "    bow = dictionary.doc2bow(sentence)\n",
    "    topic_distribution = lda_model.get_document_topics(bow)\n",
    "    topic_distributions.append(topic_distribution)\n",
    "\n",
    "# Visualize the dominant topic for each sentence\n",
    "for i, topic_dist in enumerate(topic_distributions):\n",
    "    dominant_topic = max(topic_dist, key=lambda x: x[1])\n",
    "    print(f\"Sentence {i+1}: Dominant Topic - {dominant_topic[0]}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visar ord i topics och vilken topic som är vanligast i dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.013*\"kommer\" + 0.009*\"erfarenhet\" + 0.007*\"arbeta\" + 0.006*\"söker\" + 0.006*\"tjänsten\" + 0.006*\"work\" + 0.006*\"academic\" + 0.005*\"kunskaper\" + 0.005*\"it\" + 0.005*\"vill\"')\n",
      "(1, '0.016*\"erfarenhet\" + 0.011*\"arbeta\" + 0.010*\"it\" + 0.010*\"kommer\" + 0.008*\"analytisk\" + 0.007*\"samt\" + 0.007*\"söker\" + 0.007*\"god\" + 0.006*\"förmåga\" + 0.006*\"tjänsten\"')\n",
      "(2, '0.029*\"analytisk\" + 0.016*\"förmåga\" + 0.014*\"god\" + 0.009*\"person\" + 0.008*\"samt\" + 0.008*\"ser\" + 0.007*\"strukturerad\" + 0.007*\"arbeta\" + 0.006*\"både\" + 0.006*\"söker\"')\n",
      "Most Common Dominant Topic: 2 (Frequency: 218)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Print the topics and their top keywords\n",
    "topics = lda_model.print_topics(num_words=10)\n",
    "for topic in topics:\n",
    "    print(topic)\n",
    "\n",
    "\n",
    "\n",
    "# Count the frequencies of dominant topics\n",
    "dominant_topic_freq = {}\n",
    "for topic_dist in topic_distributions:\n",
    "    dominant_topic = max(topic_dist, key=lambda x: x[1])[0]\n",
    "    dominant_topic_freq[dominant_topic] = dominant_topic_freq.get(dominant_topic, 0) + 1\n",
    "\n",
    "# Find the most common dominant topic\n",
    "most_common_topic = max(dominant_topic_freq, key=dominant_topic_freq.get)\n",
    "frequency = dominant_topic_freq[most_common_topic]\n",
    "\n",
    "print(f\"Most Common Dominant Topic: {most_common_topic} (Frequency: {frequency})\")\n",
    "\n",
    "# ..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (VIKTIG) Tar tre första orden i mest dominant topic (manuellt) och skickar in det som en sträng för att visa hur det kan se ut i en mening i dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['rekryterande', 'nämsta', 'tillsammans']\n",
      "Similarity: 0.74 Sentence: tillsammans med din chef sätter ni sedan kontinuerligt mål arbetet och utbildningsmål\n",
      "Similarity: 0.69 Sentence: hur du utvecklas framöver och vad du vill jobba med framtiden bestämmer du tillsammans med din chef och ni gör tillsammans en planering för din framtida karriär\n",
      "Similarity: 0.65 Sentence: jag som chef tillsammans med mina teamledare arbetar för att du som medarbetare skall utvecklas din vardag\n",
      "Similarity: 0.63 Sentence: på semcon får du tillsammans med ett sammansvetsat team och en lyhörd chef möjligheten att utveckla samtidens och framtidens autonoma system med våra kundbolag lokalt regionalt och globalt\n",
      "Similarity: 0.62 Sentence: du får vara med att forma din tjänst tillsammans med it chef och dina kollegor\n",
      "Similarity: 0.62 Sentence: peter engberg som är chef på it avdelningen förklarar att dem söker en social kollega som anpassar sitt och fungerar bra tillsammans med teamet och andra delar av tieto\n",
      "Similarity: 0.61 Sentence: på semcon får du tillsammans med ett sammansvetsat team och en lyhörd chef möjligheten att utveckla samtidens och framtidens mjukvara med våra kundbolag lokalt regionalt och globalt inom branschen industry\n",
      "Similarity: 0.59 Sentence: på semcon får du tillsammans med ett sammansvetsat team och en lyhörd chef möjligheten att utveckla samtidens och framtidens inbyggda system med våra kundbolag lokalt regionalt och globalt inom branschen industry\n",
      "Similarity: 0.59 Sentence: på semcon får du tillsammans med ett sammansvetsat team och en lyhörd chef möjligheten att utveckla samtidens och framtidens inbyggda system med våra kundbolag lokalt regionalt och globalt inom branschen industry\n",
      "Similarity: 0.58 Sentence: tillsammans med din chef sätter ni sedan utvecklings och utbildningsmål som kommer att följas upp kontinuerligt för att du ska fortsätta din utveckling hos oss\n",
      "Similarity: 0.55 Sentence: vpds chefer har få rapporterande till sig och du kommer tillsammans med din chef sätta upp en individuell karriärplan samt ha en löpande dialog kring din utveckling\n",
      "Similarity: 0.55 Sentence: vpds chefer har få rapporterande till sig och du kommer tillsammans med din chef sätta upp en individuell karriärplan samt ha en löpande dialog kring din utveckling\n",
      "Similarity: 0.53 Sentence: vi erbjuder såväl nationella som internationella möjligheter och tillsammans med din coachande chef bygger du din karriär så att du kan utvecklas och förverkliga dina mål och drömmar\n",
      "Similarity: 0.53 Sentence: vi erbjuder såväl nationella som internationella möjligheter och tillsammans med din coachande chef bygger du din karriär så att du kan utvecklas och förverkliga dina mål och drömmar\n",
      "Similarity: 0.53 Sentence: vi erbjuder såväl nationella som internationella möjligheter och tillsammans med din coachande chef bygger du din karriär så att du kan utvecklas och förverkliga dina mål och drömmar\n",
      "Similarity: 0.53 Sentence: vi erbjuder såväl nationella som internationella möjligheter och tillsammans med din coachande chef bygger du din karriär så att du kan utvecklas och förverkliga dina mål och drömmar\n",
      "Similarity: 0.53 Sentence: vi erbjuder såväl nationella som internationella möjligheter och tillsammans med din coachande chef bygger du din karriär så att du kan utvecklas och förverkliga dina mål och drömmar\n",
      "Similarity: 0.53 Sentence: vi erbjuder såväl nationella som internationella möjligheter och tillsammans med din coachande chef bygger du din karriär så att du kan utvecklas och förverkliga dina mål och drömmar\n",
      "Similarity: 0.53 Sentence: vi erbjuder såväl nationella som internationella möjligheter och tillsammans med din coachande chef bygger du din karriär så att du kan utvecklas och förverkliga dina mål och drömmar\n",
      "Similarity: 0.53 Sentence: vi erbjuder såväl nationella som internationella möjligheter och tillsammans med din coachande chef bygger du din karriär så att du kan utvecklas och förverkliga dina mål och drömmar\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "# Load the saved Word2Vec model\n",
    "model = Word2Vec.load('word2vec_model_desc_sentence')\n",
    "\n",
    "# Define a target sentence for similarity comparison\n",
    "target_sentence = \"rekryterande nämsta tillsammans\"\n",
    "\n",
    "# Preprocess the target sentence\n",
    "stop_words = set(stopwords.words('swedish'))  # Load Swedish stop words from NLTK\n",
    "processed_target = [word for word in simple_preprocess(target_sentence) if word not in stop_words]\n",
    "\n",
    "print(processed_target)\n",
    "\n",
    "# Find similar sentences\n",
    "similar_sentences = []\n",
    "for sentence in processed_data:\n",
    "    # Remove stop words from the sentence\n",
    "    processed_sentence = [word for word in sentence if word not in stop_words]\n",
    "    if len(sentence) > 0 and 'chef' in sentence:  # Check if 'priority_word' is present in the sentence\n",
    "        similarity = model.wv.n_similarity(processed_target, processed_sentence)\n",
    "        similar_sentences.append((sentence, similarity))\n",
    "\n",
    "\n",
    "\n",
    "# Sort the similar sentences based on similarity score\n",
    "similar_sentences = sorted(similar_sentences, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the top 5 similar sentences\n",
    "for sentence, similarity in similar_sentences[:20]:\n",
    "    original_sentence = ' '.join(sentence)\n",
    "    print(f\"Similarity: {similarity:.2f} Sentence: {original_sentence}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

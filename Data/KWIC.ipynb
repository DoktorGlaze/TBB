{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "job_ads = pd.read_json('Testfil_FINAL_2.json')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Tokeniza, ta bort stopwords och punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"# Define the keywords to search for\\nkeywords = ['drivkraft']\\n\\n# Define the number of characters to show before and after the keyword\\nwindow_size = 3\\n\\n# Iterate through each keyword\\nfor keyword in keywords:\\n    # Get the indices of all occurrences of the keyword\\n    indices = [i for i, w in enumerate(text) if w == keyword]\\n\\n    # Define a list to store the KWIC for the keyword\\n    kwic = []\\n\\n    # Generate the KWIC for each occurrence of the keyword\\n    for i in indices:\\n        left = ' '.join(text[i-window_size:i])\\n        right = ' '.join(text[i+1:i+window_size+1])\\n        print(left, text[i], right)\\n        print('-' * 50)\\n        kwic.append(left + ' ' + keyword + ' ' + right)\""
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk.text import Text\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "# Create a list of all the job ad descriptions\n",
    "texts = [t.lower() for t in job_ads['description_text'].tolist()]\n",
    "\n",
    "# Create a stop_words set\n",
    "stop_words = set(stopwords.words('swedish'))\n",
    "\n",
    "# Tokenize the descriptions and remove stopwords and punctuations\n",
    "tokens = [[w.translate(str.maketrans('', '', string.punctuation)) for w in word_tokenize(d) if not w in stop_words] for d in texts]\n",
    "\n",
    "# Create an NLTK Text object from the tokens\n",
    "text = Text([t for d in tokens for t in d])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Gör en KWIC och räkna vanligaste förekommande ord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common words in KWIC for \"stark\":\n",
      "präglas: 84\n",
      "vilja: 68\n",
      "entreprenörsanda: 66\n",
      "finns: 47\n",
      "drivkraft: 42\n",
      "gemenskap: 33\n",
      "tillväxt: 30\n",
      "efterfrågan: 25\n",
      "team: 24\n",
      "värdegrund: 23\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Define the keywords to search for\n",
    "keywords = ['stark']\n",
    "\n",
    "# Define the number of characters to show before and after the keyword\n",
    "window_size = 1\n",
    "\n",
    "for keyword in keywords:\n",
    "    # Get the indices of all occurrences of the keyword\n",
    "    indices = [i for i, w in enumerate(text) if w == keyword]\n",
    "\n",
    "    # Define a list to store the KWIC for the keyword\n",
    "    kwic = []\n",
    "\n",
    "    # Generate the KWIC for each occurrence of the keyword\n",
    "    for i in indices:\n",
    "        left = ' '.join(text[i-window_size:i])\n",
    "        right = ' '.join(text[i+1:i+window_size+1])\n",
    "        kwic.append(left + ' ' + keyword + ' ' + right)\n",
    "\n",
    "    # Identify the frequently occurring keywords in the KWIC output\n",
    "    kwic_words = [w for s in kwic for w in s.split() if w != keyword and w.isalpha()]\n",
    "    kwic_counter = Counter(kwic_words)\n",
    "    print(f'Most common words in KWIC for \"{keyword}\":')\n",
    "    for word, count in kwic_counter.most_common(10):\n",
    "        print(f'{word}: {count}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Sentiment-analys för FLERA ord."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyword: stark, Sentiment score: -0.14\n",
      "Keyword: drivkraft, Sentiment score: 0.07\n",
      "Keyword: chef, Sentiment score: 0.03\n",
      "Keyword: analys, Sentiment score: 0.04\n",
      "Keyword: analytisk, Sentiment score: 0.13\n",
      "Keyword: driven, Sentiment score: 0.15\n",
      "Keyword: individer, Sentiment score: 0.11\n",
      "Keyword: beslut, Sentiment score: 0.05\n",
      "Keyword: kompetent, Sentiment score: 0.04\n",
      "Keyword: självständig, Sentiment score: 0.10\n",
      "Average sentiment score: 0.05699618112964979\n"
     ]
    }
   ],
   "source": [
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk import word_tokenize\n",
    "from nltk.text import Text\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Load the sentiment analyzers\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Define a list to store the sentiment scores\n",
    "sentiment_scores = []\n",
    "\n",
    "# Create a list of all the job ad descriptions\n",
    "texts = [t.lower() for t in job_ads['description_text'].tolist()]\n",
    "\n",
    "# Create a stop_words set\n",
    "stop_words = set(stopwords.words('swedish'))\n",
    "\n",
    "# Tokenize the descriptions and remove stopwords and punctuations\n",
    "tokens = [[w.translate(str.maketrans('', '', string.punctuation)) for w in word_tokenize(d) if not w in stop_words] for d in texts]\n",
    "\n",
    "# Create an NLTK Text object from the tokens\n",
    "text = Text([t for d in tokens for t in d])\n",
    "\n",
    "# Define the keywords to search for\n",
    "keywords = ['stark',\n",
    "'drivkraft',\n",
    "'chef',\n",
    "'analys',\n",
    "'analytisk',\n",
    "'driven',\n",
    "'individer',\n",
    "'beslut',\n",
    "'kompetent',\n",
    "'självständig']\n",
    "\n",
    "# Define the number of characters to show before and after the keyword\n",
    "window_size = 15\n",
    "\n",
    "# Iterate through each keyword\n",
    "for keyword in keywords:\n",
    "    # Get the indices of all occurrences of the keyword\n",
    "    indices = [i for i, w in enumerate(text) if w == keyword]\n",
    "\n",
    "    # Define a list to store the KWIC for the keyword\n",
    "    kwic = []\n",
    "\n",
    "    # Generate the KWIC for each occurrence of the keyword\n",
    "    for i in indices:\n",
    "        left = ' '.join(text[i-window_size:i])\n",
    "        right = ' '.join(text[i+1:i+window_size+1])\n",
    "        kwic.append(left + ' ' + keyword + ' ' + right)\n",
    "\n",
    "    # Calculate the sentiment score for the KWICs\n",
    "    kwic_sentiment_scores = []\n",
    "    for kwic_text in kwic:\n",
    "        sia_scores = sia.polarity_scores(kwic_text)\n",
    "        tb_scores = TextBlob(kwic_text).sentiment\n",
    "        # Combine the sentiment scores from the two analyzers\n",
    "        compound_score = sia_scores['compound'] + tb_scores.polarity\n",
    "        kwic_sentiment_scores.append(compound_score)\n",
    "\n",
    "    # Calculate the average sentiment score for the keyword\n",
    "    avg_sentiment = sum(kwic_sentiment_scores) / len(kwic_sentiment_scores)\n",
    "    sentiment_scores.append(avg_sentiment)\n",
    "    print(f\"Keyword: {keyword}, Sentiment score: {avg_sentiment:.2f}\")\n",
    "\n",
    "# Calculate the average sentiment score\n",
    "avg_sentiment_score = sum(sentiment_scores) / len(sentiment_scores)\n",
    "print(\"Average sentiment score:\", avg_sentiment_score)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Sentiment-analys på ETT ord."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyword: drivkraft, Sentiment score: 0.07\n"
     ]
    }
   ],
   "source": [
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk import word_tokenize\n",
    "from nltk.text import Text\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Load the sentiment analyzers\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Define a list to store the sentiment scores\n",
    "sentiment_scores = []\n",
    "\n",
    "##### HÄR BÖRJAR KWIC #####\n",
    "\n",
    "# Create a list of all the job ad descriptions\n",
    "texts = [t.lower() for t in job_ads['description_text'].tolist()]\n",
    "\n",
    "# Create a stop_words set\n",
    "stop_words = set(stopwords.words('swedish'))\n",
    "\n",
    "# Tokenize the descriptions and remove stopwords and punctuations\n",
    "tokens = [[w.translate(str.maketrans('', '', string.punctuation)) for w in word_tokenize(d) if not w in stop_words] for d in texts]\n",
    "\n",
    "# Create an NLTK Text object from the tokens\n",
    "text = Text([t for d in tokens for t in d])\n",
    "\n",
    "# Define the keywords to search for\n",
    "keywords = ['stark']\n",
    "\n",
    "# Define the number of characters to show before and after the keyword\n",
    "window_size = 15\n",
    "\n",
    "\n",
    "# Get the indices of all occurrences of the keyword\n",
    "indices = [i for i, w in enumerate(text) if w == keyword]\n",
    "\n",
    "# Define a list to store the KWIC for the keyword\n",
    "kwic = []\n",
    "\n",
    "##### HÄR BÖRJAR SENTIMENT ANALYSEN #####\n",
    "\n",
    "# Generate the KWIC for each occurrence of the keyword\n",
    "for i in indices:\n",
    "    left = ' '.join(text[i-window_size:i])\n",
    "    right = ' '.join(text[i+1:i+window_size+1])\n",
    "    kwic.append(left + ' ' + keyword + ' ' + right)\n",
    "\n",
    "# Calculate the sentiment score for the KWICs\n",
    "kwic_sentiment_scores = []\n",
    "for kwic_text in kwic:\n",
    "    sia_scores = sia.polarity_scores(kwic_text)\n",
    "    tb_scores = TextBlob(kwic_text).sentiment\n",
    "    # Combine the sentiment scores from the two analyzers\n",
    "    compound_score = sia_scores['compound'] + tb_scores.polarity\n",
    "    kwic_sentiment_scores.append(compound_score)\n",
    "\n",
    "# Calculate the average sentiment score for the keyword\n",
    "avg_sentiment = sum(kwic_sentiment_scores) / len(kwic_sentiment_scores)\n",
    "sentiment_scores.append(avg_sentiment)\n",
    "print(f\"Keyword: {keyword}, Sentiment score: {avg_sentiment:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
